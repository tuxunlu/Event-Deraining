TRAINING:
  deterministic: False
  use_compile: False
  inference_mode: False  # If this one is True, use inference
  seed: 42
  max_epochs: 50
  
DISTRIBUTED:
  accelerator: gpu
  devices: 2
  num_nodes: 1
  strategy: auto
  gradient_clip_val: 0
  detect_anomaly: False
  
DATA:
  dataset:
    file_name: EventRainEFFT2D
    class_name: EventRainEFFT2D
    root: /fs/nexus-scratch/tuxunlu/git/event-based-deraining/dataset/efft_results
  dataloader:
    batch_size: 2
    test_batch_size: 4
    num_workers: 2
    persistent_workers: True
    pin_memory: True
    multiprocessing_context: fork
    drop_last: False
    shuffle_train: True
    shuffle_val: False
    shuffle_test: False

MODEL:
  file_name: ProgressiveFourierMamba   # Updated to match the script name from your error log
  class_name: ProgressiveFourierMamba    # Updated to match the class name we implemented
  in_chans: 1              # Matches dataset seq_len or merged channels
  out_chans: 1
  dim: 32                 # Base dimension (verified in script)
  num_blocks: [2, 2, 2, 2]    # Encoder/Decoder depths

OPTIMIZER:
  name: Adam
  arguments:
    lr: 5e-4
    weight_decay: 1e-5  # Default L2 Regularization

SCHEDULER:
  learning_rate:      # For learning rate scheduler, check document: https://docs.pytorch.org/docs/2.8/optim.html#how-to-adjust-learning-rate
    enabled: True
    name: CosineAnnealingLR
    arguments:
      T_max: 50
      eta_min: 1e-6
  
LOGGER:
  log_dir_root: lightning_logs
  experiment_name: FourierMamba2DTest_bg_identity_rain_loss_binary_mag

CHECKPOINT:
  enabled: True
  every_n_epochs: 5
  monitor: val_loss_epoch
  mode: min
  filename: "best-{epoch:03d}-{val_loss_epoch:.5f}"
  save_top_k: 1
  save_last: True