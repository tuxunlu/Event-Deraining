TRAINING:
  deterministic: False
  use_compile: False
  inference_mode: False  # If this one is True, use inference
  seed: 42
  max_epochs: 200

DISTRIBUTED:
  accelerator: gpu
  devices: 4
  num_nodes: 1
  strategy: auto

DATA:
  dataset:
    file_name: EventRainEFFT
    class_name: EventRainEFFT
    root: /fs/nexus-scratch/tuxunlu/git/event-based-deraining/dataset/efft_results
    seq_len: 5
  dataloader:
    batch_size: 2
    test_batch_size: 1
    num_workers: 8
    persistent_workers: True
    pin_memory: True
    multiprocessing_context: fork
    drop_last: False
    shuffle_train: True
    shuffle_val: False
    shuffle_test: False

MODEL:
  file_name: EventFFTMamba
  class_name: EventFFTMamba
  H: 256
  W: 256
  N: 5
  d_models: [32, 32, 32, 32, 32, 32, 32]

OPTIMIZER:
  name: Adam
  arguments:
    lr: 1e-3
    weight_decay: 1e-5  # Default L2 Regularization

SCHEDULER:
  learning_rate:      # For learning rate scheduler, check document: https://docs.pytorch.org/docs/2.8/optim.html#how-to-adjust-learning-rate
    enabled: True
    name: CosineAnnealingLR
    arguments:
      T_max: 100
      eta_min: 1e-6
  
LOGGER:
  log_dir_root: lightning_logs
  experiment_name: EventFFTMamba_with_final_skip

CHECKPOINT:
  enabled: True
  every_n_epochs: 1
  monitor: val_loss_epoch
  mode: min
  filename: "best-{epoch:03d}-{val_loss_epoch:.5f}"
  save_top_k: 1
  save_last: True
